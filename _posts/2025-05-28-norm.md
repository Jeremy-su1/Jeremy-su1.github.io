---
title: "임베딩 벡터의 정규화와 Cosine Similarity의 관계"
date: 2025-05-19
categories: [ML, DeepLearning,NLP]
tags: [Embedding, Normalization, Cosine Similarity, Contrastive Learning, CLIP]
---

## 임베딩 벡터 정규화와 코사인 유사도(Cosine Similarity)의 관계

딥러닝에서 임베딩 벡터는 텍스트나 이미지와 같은 입력 데이터를 고차원 공간에 표현한 결과입니다. 이 임베딩 벡터의 비교는 매우 중요하며, **정규화(norm) 과정**은 이 비교의 정확도에 큰 영향을 줍니다.

---

### 1. Norm이란?

**Norm(노름)**은 벡터의 크기(길이)를 의미하며, 가장 일반적으로 사용되는 것은 **L2 노름**입니다.


$\|x\|_2 = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}$

---

### 2. 임베딩 벡터를 정규화하는 이유

임베딩 벡터를 L2 노름으로 나누면, 모든 벡터의 크기가 1로 맞춰집니다:


$\hat{x} = \frac{x}{\|x\|_2}$

이렇게 정규화하면 벡터의 **방향만 유지**되고, **크기(스케일)**의 영향은 제거됩니다. 즉, 임베딩 벡터 간의 비교가 방향 중심으로 이루어집니다.

---

### 3. Cosine Similarity와의 관계

Cosine Similarity는 두 벡터 사이의 방향 유사도를 측정하는 지표입니다:


$\cos(\theta) = \frac{x \cdot y}{\|x\|\|y\|}$


벡터들을 L2 정규화하면 $\|x\| = \|y\| = 1$이므로,

$\cos(\theta) = x \cdot y$


즉, **정규화된 벡터의 내적(dot product)이 곧 cosine similarity**가 됩니다. 이는 계산을 단순화시킬 뿐만 아니라, **벡터의 길이 차이로 인한 왜곡을 방지**할 수 있습니다.

---

### 4. 실전 적용: CLIP과 Contrastive Learning

OpenAI의 **CLIP** 모델은 이미지와 텍스트의 임베딩을 비교하여 연관성을 학습합니다. 이 과정에서 L2 정규화는 다음과 같은 장점을 제공합니다:

- 다양한 입력이 생성하는 벡터 크기의 차이를 제거
- cosine similarity 기반의 loss 계산을 안정화
- contrastive learning에서 보다 일관된 학습 제공

또한, Reranking 과정에서도 임베딩 벡터의 크기 차이 없이 정확한 유사도 비교가 가능합니다.

---

### ✅ 결론

임베딩 벡터 정규화는 단순한 수학적 변환처럼 보일 수 있지만, 실전에서는 매우 중요한 역할을 합니다. 특히 cosine similarity를 기반으로 한 시스템에서는 **L2 정규화를 통해 유사도 계산의 신뢰성을 확보**할 수 있으며, CLIP과 같은 최신 모델의 핵심 구성 요소이기도 합니다.

---