---
title: "임베딩 벡터의 정규화와 Cosine Similarity의 관계"
date: 2025-05-19
categories: [ML, DeepLearning,NLP]
tags: [Embedding, Normalization, Cosine Similarity, Contrastive Learning, CLIP]
toc: true
toc_sticky: true
Typora-root-url: ../
author_profile: false
sidebar:
  nav: "docs"
search: true
math: true
---

## 임베딩 벡터 정규화와 코사인 유사도(Cosine Similarity)의 관계

딥러닝에서 임베딩 벡터는 텍스트나 이미지와 같은 입력 데이터를 고차원 공간에 표현한 결과입니다. 이 임베딩 벡터의 비교는 매우 중요하며, **정규화(norm) 과정**은 이 비교의 정확도에 큰 영향을 줍니다.

---
### 1. Norm이란?

**Norm(노름)**은 벡터의 크기(길이)를 의미하며, 벡터 공간에서 점 간 거리를 정의하는 척도입니다. 대표적인 Norm에는 **L1 Norm**과 **L2 Norm**이 있습니다.

#### 🔹 L1 Norm (맨해튼 거리, Manhattan Distance)

L1 Norm은 벡터의 각 요소 절댓값을 모두 더한 값입니다:


$\|x\|_1 = |x_1| + |x_2| + \cdots + |x_n|$


- 도로처럼 수직·수평으로만 이동할 수 있는 경우에 해당 (택시 거리)
- 희소한 벡터(sparse vector)에서 주로 사용
- 예: Lasso Regression에서 정규화 항으로 사용

#### 🔹 L2 Norm (유클리디안 거리, Euclidean Distance)

L2 Norm은 가장 널리 쓰이는 방식으로, 벡터의 각 요소를 제곱한 뒤 루트를 씌운 값입니다:


$\|x\|_2 = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}$


- 가장 직관적인 거리 개념 (직선 거리)
- 임베딩 벡터의 크기 측정 시 자주 사용
- 예: Ridge Regression에서 정규화 항으로 사용

---

### 🔍 L1 Norm vs L2 Norm 비교 요약

| 구분 | L1 Norm | L2 Norm |
|------|---------|---------|
| 의미 | 절대값의 합 | 제곱합의 루트 |
| 별칭 | Manhattan Distance | Euclidean Distance |
| 특징 | 희소성 유도 (Sparse) | 부드러운 패널티(Smooth) |
| 사용 예시 | Lasso Regression, 일부 Autoencoder | Regularization, Embedding 비교 |
| Gradient | Sharp (절대값), 변화가 급격함 | Smooth, 연산이 안정적임 |

---

정리하면,  
- **L1 Norm**은 작은 수치들을 완전히 0으로 만들어 희소한 결과를 유도할 수 있고,  
- **L2 Norm**은 크기를 점진적으로 줄여 전체적인 안정성과 연속성을 유지합니다.  

임베딩 벡터의 정규화에는 보통 **L2 Norm**이 사용되며, 이는 cosine similarity 계산과 직접적인 연관이 있습니다.
---

### 2. 임베딩 벡터를 정규화하는 이유

임베딩 벡터를 L2 노름으로 나누면, 모든 벡터의 크기가 1로 맞춰집니다:


$\hat{x} = \frac{x}{\|x\|_2}$

이렇게 정규화하면 벡터의 **방향만 유지**되고, **크기(스케일)**의 영향은 제거됩니다. 즉, 임베딩 벡터 간의 비교가 방향 중심으로 이루어집니다.

---

### 3. Cosine Similarity와의 관계

Cosine Similarity는 두 벡터 사이의 방향 유사도를 측정하는 지표입니다:


$\cos(\theta) = \frac{x \cdot y}{\|x\|\|y\|}$


벡터들을 L2 정규화하면 $\|x\| = \|y\| = 1$이므로,

$\cos(\theta) = x \cdot y$


즉, **정규화된 벡터의 내적(dot product)이 곧 cosine similarity**가 됩니다. 이는 계산을 단순화시킬 뿐만 아니라, **벡터의 길이 차이로 인한 왜곡을 방지**할 수 있습니다.
