---
title: "딥러닝의 핵심 함수, Softmax 정규화란?"
date: 2025-05-19
categories: [DeepLearning, NLP]
tags: [softmax, 정규화, 확률분포, 신경망, output layer]
toc: true
toc_sticky: true
Typora-root-url: ../
author_profile: false
sidebar:
  nav: "docs"
search: true
---

## 🔍 Softmax 정규화란?

딥러닝 모델, 특히 자연어처리(NLP)나 분류(classification) 문제에서 자주 등장하는 함수가 바로 **Softmax 정규화**입니다.  
Softmax는 모델의 출력 값을 **확률 분포(probability distribution)**로 변환해주는 함수로, 마지막 출력층에서 자주 사용됩니다.

---

### ✅ Softmax 수식

Softmax 함수는 다음과 같이 정의됩니다:

$\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}$


- $z_i$: 입력 벡터의 i번째 요소  
- $e^{z_i}$: 지수 함수 (값을 양수로 만들고 크기 차이를 확대함)  
- 전체 합 $\sum e^{z_j}$로 나누어 **총합이 1인 확률분포**로 정규화

---

### ✅ 왜 Softmax가 중요한가요?

Softmax는 다음과 같은 이유로 자주 사용됩니다:

- **출력을 확률처럼 해석 가능**  
  → 예: `[0.1, 0.7, 0.2]` → 2번째 클래스가 나올 확률이 가장 높음

- **분류 모델의 출력층에 적합**  
  → LLM이나 이미지 분류 등에서 가장 가능성 높은 클래스를 선택할 수 있음

- **Cross-Entropy Loss와 잘 어울림**  
  → Softmax 출력은 Cross-Entropy 손실 함수와 함께 사용되며, 학습이 빠르고 안정적

---

### ✅ Python 코드 예제

```python
import numpy as np

def softmax(x):
    e_x = np.exp(x - np.max(x))  # overflow 방지
    return e_x / e_x.sum()

logits = np.array([2.0, 1.0, 0.1])
probs = softmax(logits)

print(probs)  # 출력: [0.659, 0.242, 0.099]
```

✅ 주의할 점
입력 값이 클 경우 $e^{Z_i}$ 가 너무 커져 overflow 문제가 발생할 수 있음
→ np.max(x)를 빼는 방식으로 보정 (log-sum-exp trick)

모든 값을 양수로 만들고 총합이 1이 되므로, 확률 분포로 해석 가능

✅ LLM과 Softmax
LLM의 출력층에서도 Softmax는 핵심 역할을 합니다.

출력 벡터(로짓, logits)는 단어 사전 전체에 대한 스코어

이 스코어에 Softmax를 적용하면, 각 단어가 다음에 등장할 확률이 계산됨

가장 높은 확률의 단어를 샘플링하거나, Top-k / Top-p 방식으로 선택

